{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"50\" src=\"https://carbonplan-assets.s3.amazonaws.com/monogram/dark-small.png\" style=\"margin-left:0px;margin-top:20px\"/>\n",
    "\n",
    "# Buffer Pool Analysis\n",
    "\n",
    "_by Grayson Badgley, September 17, 2020_\n",
    "\n",
    "This notebook performs analysis of the\n",
    "[California Air Resources Board carbon credit issuance database](https://ww2.arb.ca.gov/our-work/programs/compliance-offset-program/arb-offset-credit-issuance)\n",
    "to quantify the number of ARB offset credits issued to the Warm Springs Phase I\n",
    "(ACR260/CAFR5214) improved forest management offset project.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "The Analysis consists of two parts: cleaning the data and calculating simple\n",
    "summary statistics from the CARB issuance table.\n",
    "\n",
    "## References\n",
    "\n",
    "- CARB issuance database. Available online\n",
    "  [https://ww3.arb.ca.gov/cc/capandtrade/offsets/issuance/arboc_issuance.xlsx].\n",
    "- CARB quarterly compliance report (Q2 2020). Available online\n",
    "  [https://ww2.arb.ca.gov/sites/default/files/2020-07/2020_q2_complianceinstrumentreport.pdf].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from carbonplan_styles.colors import colors\n",
    "from carbonplan_styles.mpl import set_theme\n",
    "\n",
    "# set options\n",
    "set_theme(style=\"carbonplan_light\")\n",
    "c = colors(\"carbonplan_light\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\n",
    "    \"https://carbonplan-articles.s3.us-west-2.amazonaws.com/offset-project-fire/arboc_issuance.xlsx\",\n",
    "    sheet_name=3,\n",
    "    engine=\"openpyxl\",\n",
    ")\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data\n",
    "\n",
    "We note that two of the the Forest projects are reforestation and have \"Forest\n",
    "Buffer Contributions\" of \"reforest defer\" -- they haven't been issued any\n",
    "credits so we will exclude them from the rest of the analysis.\n",
    "\n",
    "We also note that one project has received zero credits -- so we will also\n",
    "remove that one in order to be conservative.\n",
    "\n",
    "Finally, we subset the table to just forests, because we want to understand\n",
    "those projects and their relationship to the _forest_ buffer pool.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Forest Buffer Account Contribution\"].apply(lambda x: isinstance(x, str))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df[\"Forest Buffer Account Contribution\"].apply(lambda x: isinstance(x, str))]\n",
    "df = df[df[\"ARB Offset Credits Issued\"] > 0]\n",
    "forest_df = df[df[\"Project Type\"] == \"Forest\"]\n",
    "\n",
    "display(forest_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Action/Compliance\n",
    "\n",
    "We note that the Early Action (EA) and Compliance (COP) contriubte to buffer\n",
    "pool, so our default assumption is to include both project types in the\n",
    "analysis. However, we also know that some projects converted over from the Early\n",
    "Action program to the Compliance phase. Because we want to make sure we don't\n",
    "double count, we'll check to see if any OPR Project IDs are listed as _both_ EA\n",
    "and COP:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(forest_df[\"Early Action/ Compliance\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    forest_df.groupby(\"OPR Project ID\")[\"Early Action/ Compliance\"]\n",
    "    .nunique()\n",
    "    .value_counts()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid Double Counting\n",
    "\n",
    "So, as we can see above, all projects just have one categorization: either EA or\n",
    "COP. To ensure we don't double count a project, we'll remove EA projects from\n",
    "the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_df = forest_df[forest_df[\"Early Action/ Compliance\"] == \"COP\"]\n",
    "display(forest_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total number of forest projects\n",
    "\n",
    "Having removed a) early action projects and b) projects that have not been\n",
    "issued ARB Offset Credits, there are 98 forest carbon projects in the issuance\n",
    "database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_projects = forest_df[\"OPR Project ID\"].nunique()\n",
    "print(\"Number of CARB Forest Projects: %d\" % n_projects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm Springs Phase I (ACR260/CAFR5214)\n",
    "\n",
    "We're interested in understanding how many credits have been issued to ACR260,\n",
    "the project in Oregon that the Lionshead Fire burned. Let's take a closer look\n",
    "at the records asscoiated with `OPR PROJECT ID == 'ACR260'`.\n",
    "\n",
    "The ARB excel file records three reporting periods during which ACR260 was\n",
    "issued credits. For each entry below, I double checked the\n",
    "`Reporting Period Start Date`, `Reporting Period End Date`, and\n",
    "`ARB Offset Credits Issued` against the publically available project\n",
    "documentation from the\n",
    "[ACR web portal](https://acr2.apx.com/mymodule/reg/TabDocuments.asp?r=111&ad=Prpt&act=update&type=PRO&aProj=pub&tablename=doc&id1=260).\n",
    "The values listed below exactly match the project ACR Annual OPDR documentation.\n",
    "It's worth noting that ACR260 also reported activity for a reporting period\n",
    "starting 2016-09-28 and ending 2017-09-27, but no credits were ultimately issued\n",
    "(which is noted in the issuance database, but not shown here because we excluded\n",
    "all entries with `ARB Offset Credits Issued <= 0`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_id = \"ACR260\"\n",
    "forest_df[forest_df[\"OPR Project ID\"] == proj_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How big is ACR260?\n",
    "\n",
    "ACR260 is a pretty big project -- it's the 14th largest of the 98 compliance\n",
    "period forest projects contained with the issuance database. ACR260 is also the\n",
    "largest in forest project in Oregon.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_credits = forest_df[\"ARB Offset Credits Issued\"].sum()\n",
    "per_proj_credits = forest_df.groupby(\"OPR Project ID\")[\n",
    "    \"ARB Offset Credits Issued\"\n",
    "].sum()\n",
    "\n",
    "perc_credits = per_proj_credits / total_credits * 100\n",
    "\n",
    "proj_state_map = (\n",
    "    forest_df.groupby(\"OPR Project ID\").State.max().to_dict()\n",
    ")  # max() makes 1:1 mapping of opr_id to state\n",
    "perc_credits = perc_credits.sort_values(ascending=False).to_frame()\n",
    "\n",
    "perc_credits[\"state\"] = perc_credits.index.map(proj_state_map)\n",
    "perc_credits = perc_credits.rename(\n",
    "    # clean up column title\n",
    "    columns={\"ARB Offset Credits Issued\": \"Percent Forest ARB Offset Credits Issued\"}\n",
    ")\n",
    "\n",
    "proj_loc = perc_credits.index.get_loc(proj_id)\n",
    "display(perc_credits[: proj_loc + 1])  # +1 to include ACR260, otherwise off by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While deriving the above ordering, I realized that it's absolutely the case that\n",
    "a big project could have occured during the Early Action period and potentially\n",
    "affect the ordering shown above. To check this, let's just quick re-do the above\n",
    "table but this time with both EA and COP projects. As it turns out, CAR730 was a\n",
    "big project in California that doesn't seem to have transfered over to the\n",
    "complicance period. So we can still say that ACR260 is the biggest in Oregon,\n",
    "but it's probably better to say that its \"among the 15 largest\" forest projects!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_proj_credits = (\n",
    "    df.loc[df[\"Project Type\"] == \"Forest\"]\n",
    "    .groupby(\"OPR Project ID\")[\"ARB Offset Credits Issued\"]\n",
    "    .sum()\n",
    ")\n",
    "\n",
    "proj_state_map = (\n",
    "    df.groupby(\"OPR Project ID\").State.max().to_dict()\n",
    ")  # max() makes 1:1 mapping of opr_id to state\n",
    "sorted_credits = per_proj_credits.sort_values(ascending=False).to_frame()\n",
    "\n",
    "sorted_credits[\"state\"] = sorted_credits.index.map(proj_state_map)\n",
    "sorted_credits = sorted_credits.rename(\n",
    "    # clean up column title\n",
    "    columns={\"ARB Offset Credits Issued\": \"Percent Forest ARB Offset Credits Issued\"}\n",
    ")\n",
    "\n",
    "proj_loc = sorted_credits.index.get_loc(proj_id)\n",
    "display(sorted_credits[: proj_loc + 1])  # +1 to include ACR260, otherwise off by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size relative to the buffer pool\n",
    "\n",
    "The next exercise is to understand the credits issued to ACR260 in context of\n",
    "the _remaining_ credits within the buffer pool. The issuance database lists all\n",
    "ARB offset credits issued and the number of those credits that are set aside in\n",
    "the forest buffer account. So summing `Forest Buffer Account Contribution` would\n",
    "give us the _total_ number of credits ever deposited in the buffer pool.\n",
    "However, the buffer pool has been drawn upon in the past. So for our statistics,\n",
    "we're going to use the Q2 complicance report mentioned above. Footnote `+`\n",
    "reports that the forest buffer account, as of Q2 2020, had 24,079,774 credits.\n",
    "\n",
    "From there, it's just a matter of summing up `ARB Offset Credits Issued` for\n",
    "ACR260\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_buffer_pool = 24_079_774\n",
    "proj_credits = forest_df[forest_df[\"OPR Project ID\"] == proj_id][\n",
    "    \"ARB Offset Credits Issued\"\n",
    "].sum()\n",
    "proj_frac = proj_credits / total_buffer_pool * 100\n",
    "\n",
    "print(f\"Remaining buffer pool (Q2 2020): {total_buffer_pool}\")\n",
    "print(f\"{proj_id} Total Issued Credits: {proj_credits}\")\n",
    "print(f\"ACR260 as a fraction of Buffer Pool: {proj_frac:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double Check Offset Issued vs Buffer Pool\n",
    "\n",
    "We also wanted to make sure that `ARB Offset Credits Issued` represented 100\n",
    "percent of the credits generated by the project, including both the credits\n",
    "awarded to the offset project operator and the credits reserved for the buffer\n",
    "pool. To double check that this thinking is correct, we calculate\n",
    "`buffer_contrib` as the ratio of `Forest Buffer Account Contribution` and\n",
    "`ARB Offset Credits Issued`. If `ARB Offset Credits Issued` represents the full\n",
    "allocation of credits generated by a project (prior to some credits being set\n",
    "aside for the buffer pool), that ratio should range mostly between 12 and 20\n",
    "percent, which is indeed the case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_df.loc[:, \"buffer_contrib\"] = (\n",
    "    forest_df[\"Forest Buffer Account Contribution\"]\n",
    "    / forest_df[\"ARB Offset Credits Issued\"]\n",
    ").copy()\n",
    "display(forest_df[\"buffer_contrib\"].astype(float).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = forest_df[\"buffer_contrib\"].hist(color=c[\"red\"])\n",
    "g.grid(None)\n",
    "g.spines[\"top\"].set_visible(False)\n",
    "g.spines[\"right\"].set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
